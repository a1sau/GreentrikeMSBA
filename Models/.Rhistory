build.accuracy.df <- data.frame(k = seq(1, k.num, 1), accuracy = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=i)
build.accuracy.df[i,2] <- confusionMatrix(knn.pred, valid.norm.df$Score)$overall[1]
}
max(build.accuracy.df)
build.accuracy.df
knn.model<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=7)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$Score))
#run KNN tests to find best k
k.num = nrow(train.df)
build.accuracy.df <- data.frame(k = seq(1, k.num, 1), accuracy = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=i)
build.accuracy.df[i,2] <- confusionMatrix(knn.pred, valid.norm.df$Score)$overall[1]
}
max(build.accuracy.df)
build.accuracy.df
knn.model<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=7)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$Score))
#Split and normalize data
train_index <- sample(row.names(build.scores),0.7*dim(build.scores)[1])
valid_index <- setdiff(row.names(build.scores), train_index)
train.df <- build.scores[train_index,]
valid.df <- build.scores[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,4:5], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 4:5] <- predict(norm.values, train.df[,4:5])
valid.norm.df[, 4:5] <- predict(norm.values, valid.df[,4:5])
#Look at and clean data
build.scores <- building.scores
build.scores$SquareFeet <- (ifelse(is.na(build.scores$SquareFeet),0,build.scores$SquareFeet))
build.scores$Price <- ifelse(is.na(build.scores$Price),100000000,build.scores$Price)
build.scores$Score <- as.factor(build.scores$Score)
build.scores$Sale_Type[build.scores$Sale_Type =="OwnerUser"] <- "Owner User"
build.scores$Sale_Type[build.scores$Sale_Type =="Investment or Owner User"] <- "Both"
build.scores$Sale_Type[build.scores$Sale_Type =="InvestmentorOwnerUser"] <- "Both"
build_class <- as.data.frame(dummy.code(build.scores$Building_Class))
sale_type <- as.data.frame(dummy.code(build.scores$Sale_Type))
#remove original and replace with dummy var.
colnames(build.scores)
build.scores <- build.scores[,-c(6:7)]
build.scores <- cbind(build.scores, build_class, sale_type)
#Split and normalize data
train_index <- sample(row.names(build.scores),0.7*dim(build.scores)[1])
valid_index <- setdiff(row.names(build.scores), train_index)
train.df <- build.scores[train_index,]
valid.df <- build.scores[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,4:5], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 4:5] <- predict(norm.values, train.df[,4:5])
valid.norm.df[, 4:5] <- predict(norm.values, valid.df[,4:5])
#run KNN tests to find best k
k.num = nrow(train.df)
build.accuracy.df <- data.frame(k = seq(1, k.num, 1), accuracy = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=i)
build.accuracy.df[i,2] <- confusionMatrix(knn.pred, valid.norm.df$Score)$overall[1]
}
max(build.accuracy.df)
build.accuracy.df
knn.model<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=7)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$Score))
#Normalize and prepare entire scored set
build.norm.scores <- rbind(train.norm.df, valid.norm.df)
new.buildings$SquareFeet <- (ifelse(is.na(new.buildings$SquareFeet),0,new.buildings$SquareFeet))
new.buildings$Price  <- ifelse(is.na(new.buildings$Price),100000000,new.buildings$Price)
new.buildings$Sale_Type[new.buildings$Sale_Type =="OwnerUser"] <- "Owner User"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment or Owner User"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="InvestmentorOwnerUser"] <- "Both"
build_class.new <- as.data.frame(dummy.code(new.buildings$Building_Class))
sale_type.new <- as.data.frame(dummy.code(new.buildings$Sale_Type))
new.buildings <- new.buildings[,-c(5,6)]
new.buildings <- cbind(new.buildings, build_class.new,sale_type.new)
new.buildings.norm <- new.buildings
new.buildings.norm[,3:4] <- predict(norm.values, new.buildings[3:4])
new.buildings.norm
build.norm.scores[,c(1,2,3,4,5,8,7,6)]
build.norm.scores[,c(9,11,10,12)]
first <- build.norm.scores[,c(1,2,3,4,5,8,7,6)]
#first$A <- 0
second <-build.norm.scores[,c(9,11,10,12)]
build.norm.scores <- cbind(first, second)
build.norm.scores
#add in scored buildings to our new dataset.
new.buildings.norm.t <- rbind(new.buildings.norm ,build.norm.scores[,c(1,3:13)])
build.norm.scores[,c(1,3:13)])
build.norm.scores[,c(1,3:12)])
build.norm.scores
build.norm.scores[,c(1,3:12)]
#add in scored buildings to our new dataset.
new.buildings.norm.t <- rbind(new.buildings.norm ,build.norm.scores[,c(1,3:12)])
new.buildings.norm
build.norm.scores[,c(9,11,10,12)]
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment NNN"] <- "Investment"
new.buildings <- dbGetQuery(con, 'SELECT B."CS_ID", B."City", B."Price", B."SquareFeet", B."Building_Class",  B."Sale_Type"
FROM "Building" B
LEFT JOIN "Building_Score" BS on B."CS_ID" = BS.cs_id
WHERE "Score" IS NULL;')
new.buildings$SquareFeet <- (ifelse(is.na(new.buildings$SquareFeet),0,new.buildings$SquareFeet))
new.buildings$Price  <- ifelse(is.na(new.buildings$Price),100000000,new.buildings$Price)
new.buildings$Sale_Type[new.buildings$Sale_Type =="OwnerUser"] <- "Owner User"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment or Owner User"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="InvestmentorOwnerUser"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment NNN"] <- "Investment"
build_class.new <- as.data.frame(dummy.code(new.buildings$Building_Class))
sale_type.new <- as.data.frame(dummy.code(new.buildings$Sale_Type))
new.buildings <- new.buildings[,-c(5,6)]
new.buildings <- cbind(new.buildings, build_class.new,sale_type.new)
new.buildings.norm <- new.buildings
new.buildings.norm[,3:4] <- predict(norm.values, new.buildings[3:4])
new.buildings.norm
new.buildings <- dbGetQuery(con, 'SELECT B."CS_ID", B."City", B."Price", B."SquareFeet", B."Building_Class",  B."Sale_Type"
FROM "Building" B
LEFT JOIN "Building_Score" BS on B."CS_ID" = BS.cs_id
WHERE "Score" IS NULL;')
new.buildings$SquareFeet <- (ifelse(is.na(new.buildings$SquareFeet),0,new.buildings$SquareFeet))
new.buildings$Price  <- ifelse(is.na(new.buildings$Price),100000000,new.buildings$Price)
new.buildings$Sale_Type[new.buildings$Sale_Type =="OwnerUser"] <- "Owner User"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment or Owner User"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="InvestmentorOwnerUser"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment NNN"] <- "Investment"
build_class.new <- as.data.frame(dummy.code(new.buildings$Building_Class))
sale_type.new <- as.data.frame(dummy.code(new.buildings$Sale_Type))
new.buildings <- new.buildings[,-c(5,6)]
new.buildings <- cbind(new.buildings, build_class.new,sale_type.new)
new.buildings.norm <- new.buildings
new.buildings.norm[,3:4] <- predict(norm.values, new.buildings[3:4])
new.buildings.norm
new.buildings
new.buildings.test <- new.buildings[,1:8] + new.buildings$Investment+ new.buildings$Both+new.buildings$`Owner User`
new.buildings.norm
build.norm.scores
build.norm.scores
build.norm.scores[,c(1,3:12)]
new.buildings.norm
new.buildings.norm[,c(1:9,12,11)]
new.buildings.norm
new.buildings.norm[,c(1:8,11,9,10)]
#add in scored buildings to our new dataset.
new.buildings.norm.t <- rbind(new.buildings.norm[,c(1:8,11,9,10)] ,build.norm.scores[,c(1,3:12)])
new.buildings.norm.t[,3:12]
new.buildings.norm.t[,2:11]
new.buildings.norm.t
#Run KNN on new records
Model_id <- "Build_KNN_v.4"
knn.pred.new<- class::knn(train = build.norm.scores[,4:13], test = new.buildings.norm.t[,3:11], cl = build.norm.scores[,2],k=7)
build.norm.scores
knn.pred.new<- class::knn(train = build.norm.scores[,4:12], test = new.buildings.norm.t[,3:11], cl = build.norm.scores[,2],k=7)
summary(knn.pred.new)
knn.pred.new
new.buildings.norm.t[101:123,]
new.buildings.norm.t[103:123,]
valid.norm.df
new.buildings.norm.t[103:123,]
valid.norm.df
confusionMatrix(knn.pred.new[103:123,], valid.norm.df$Score)
confusionMatrix(knn.pred.new[103:123], valid.norm.df$Score)
View(building.scores)
build.norm.scores
View(new.buildings.norm.t)
new.buildings.norm.t[56:123,]
confusionMatrix(knn.pred.new[56:123], valid.norm.df$Score)
build.norm.scores
confusionMatrix(knn.pred.new[56:123], build.norm.scores$Score)
outscore <- as.data.frame(knn.pred.new)
output.csv <- cbind(new.buildings.norm$CS_ID, Model_id, outscore, date())
output.csv <- cbind(new.buildings.norm.t$CS_ID, Model_id, outscore, date())
setnames(output.csv, (c( "cs_id", "model_id", "score", "date_calculated")))
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.3_output.csv",row.names = FALSE)
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
outscore <- as.data.frame(knn.pred.new)
output.csv <- cbind(new.buildings.norm.t$CS_ID, Model_id, outscore, date())
setnames(output.csv, (c( "cs_id", "model_id", "score", "date_calculated")))
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
outscore <- as.data.frame(knn.pred.new)
output.csv <- cbind(new.buildings.norm.t$CS_ID, Model_id, outscore, date())
setnames(output.csv, (c( "cs_id", "model_id", "score", "date_calculated")))
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
outscore <- as.data.frame(knn.pred.new)
output.csv <- cbind(new.buildings.norm.t$CS_ID, Model_id, outscore, date())
setnames(output.csv, (c( "cs_id", "model_id", "score", "date_calculated")))
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(DBI)
library(odbc)
library(data.table)
library(caret)
library(FNN)
library(class)
library(psych)
# Connect to database
con <- DBI::dbConnect(odbc::odbc(),
driver = "PostgreSQL Unicode(x64)",
database = "TEST",
UID      = rstudioapi::askForPassword("Database user"),
PWD      = rstudioapi::askForPassword("Database password"),
server = "greentrike.cfvgdrxonjze.us-west-2.rds.amazonaws.com",
port = 5432)
#Get list of building that have scores for them
building.scores <- dbGetQuery(con, 'SELECT B."CS_ID", BS."Score", "City", "Price", "SquareFeet", "Building_Class", "Sale_Type"
FROM "Building" B
RIGHT JOIN "Building_Score" BS on B."CS_ID" = BS.cs_id;')
new.buildings <- dbGetQuery(con, 'SELECT B."CS_ID", B."City", B."Price", B."SquareFeet", B."Building_Class",  B."Sale_Type"
FROM "Building" B
LEFT JOIN "Building_Score" BS on B."CS_ID" = BS.cs_id
WHERE "Score" IS NULL;')
#Look at and clean data
build.scores <- building.scores
build.scores$SquareFeet <- (ifelse(is.na(build.scores$SquareFeet),0,build.scores$SquareFeet))
build.scores$Price <- ifelse(is.na(build.scores$Price),100000000,build.scores$Price)
build.scores$Score <- as.factor(build.scores$Score)
build.scores$Sale_Type[build.scores$Sale_Type =="OwnerUser"] <- "Owner User"
build.scores$Sale_Type[build.scores$Sale_Type =="Investment or Owner User"] <- "Both"
build.scores$Sale_Type[build.scores$Sale_Type =="InvestmentorOwnerUser"] <- "Both"
build_class <- as.data.frame(dummy.code(build.scores$Building_Class))
sale_type <- as.data.frame(dummy.code(build.scores$Sale_Type))
#remove original and replace with dummy var.
colnames(build.scores)
build.scores <- build.scores[,-c(6:7)]
build.scores <- cbind(build.scores, build_class, sale_type)
#Split and normalize data
train_index <- sample(row.names(build.scores),0.7*dim(build.scores)[1])
valid_index <- setdiff(row.names(build.scores), train_index)
train.df <- build.scores[train_index,]
valid.df <- build.scores[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,4:5], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 4:5] <- predict(norm.values, train.df[,4:5])
valid.norm.df[, 4:5] <- predict(norm.values, valid.df[,4:5])
#run KNN tests to find best k
k.num = nrow(train.df)
build.accuracy.df <- data.frame(k = seq(1, k.num, 1), accuracy = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=i)
build.accuracy.df[i,2] <- confusionMatrix(knn.pred, valid.norm.df$Score)$overall[1]
}
max(build.accuracy.df)
build.accuracy.df
knn.model<- class::knn(train = train.norm.df[,4:12], test = valid.norm.df[,4:12], cl = train.norm.df[,2],k=7)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$Score))
#Normalize and prepare entire scored set
build.norm.scores <- rbind(train.norm.df, valid.norm.df)
#Normalize and prepare new.records
#square feet to 0, dummy var on sale_type, property_type and Build_class
# set Sale_type to three categories
# Set long property_type office
new.buildings$SquareFeet <- (ifelse(is.na(new.buildings$SquareFeet),0,new.buildings$SquareFeet))
new.buildings$Price  <- ifelse(is.na(new.buildings$Price),100000000,new.buildings$Price)
new.buildings$Sale_Type[new.buildings$Sale_Type =="OwnerUser"] <- "Owner User"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment or Owner User"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="InvestmentorOwnerUser"] <- "Both"
new.buildings$Sale_Type[new.buildings$Sale_Type =="Investment NNN"] <- "Investment"
build_class.new <- as.data.frame(dummy.code(new.buildings$Building_Class))
sale_type.new <- as.data.frame(dummy.code(new.buildings$Sale_Type))
#remove original and replace with dummy var.
new.buildings <- new.buildings[,-c(5,6)]
new.buildings <- cbind(new.buildings, build_class.new,sale_type.new)
new.buildings.norm <- new.buildings
new.buildings.norm[,3:4] <- predict(norm.values, new.buildings[3:4])
new.buildings.norm
first <- build.norm.scores[,c(1,2,3,4,5,8,7,6)]
#first$A <- 0
second <-build.norm.scores[,c(9,11,10,12)]
build.norm.scores <- cbind(first, second)
build.norm.scores
#add in scored buildings to our new dataset.
new.buildings.norm.t <- rbind(new.buildings.norm[,c(1:8,11,9,10)] ,build.norm.scores[,c(1,3:12)])
#Run KNN on new records
Model_id <- "Build_KNN_v.4"
knn.pred.new<- class::knn(train = build.norm.scores[,4:12], test = new.buildings.norm.t[,3:11], cl = build.norm.scores[,2],k=7)
summary(knn.pred.new)
new.buildings.norm.t[56:123,]
valid.norm.df
confusionMatrix(knn.pred.new[56:123], build.norm.scores$Score)
outscore <- as.data.frame(knn.pred.new)
output.csv <- cbind(new.buildings.norm.t$CS_ID, Model_id, outscore, date())
setnames(output.csv, (c( "cs_id", "model_id", "score", "date_calculated")))
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
RMSE(knn.pred.new[56:123], build.norm.scores$Score)
knn.pred.new[56:123]
build.norm.scores$Score
knn.pred.new[56:123]
RMSE(knn.pred.new[56:123], build.norm.scores$Score)
RMSE(as.numeric(knn.pred.new[56:123]), as.numeric(build.norm.scores$Score))
confusionMatrix(knn.pred.new[56:123], build.norm.scores$Score)
knn.model
knn.model.train<- class::knn(train = train.norm.df[,4:12], test = train.norm.df[,4:12], cl = train.norm.df[,2],k=7)
knn.model.train
confusionMatrix(as.factor(knn.model.train), as.factor(train.norm.df$Score))
output.csv <- cbind(new.buildings.norm.t$CS_ID, Model_id, outscore, "2021-03-12")
setnames(output.csv, (c( "cs_id", "model_id", "score", "date_calculated")))
write.csv(output.csv,"C:/Users/Benjamin/Documents/UWTacoma/MSBA/Aplied Project with Greentrike/KNN MODELS\\Building_KNN_v.4_output.csv",row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(DBI)
library(odbc)
library(data.table)
library(caret)
library(FNN)
library(class)
library(psych)
# Connect to database
con <- DBI::dbConnect(odbc::odbc(),
driver = "PostgreSQL Unicode(x64)",
database = "TEST",
UID      = rstudioapi::askForPassword("Database user"),
PWD      = rstudioapi::askForPassword("Database password"),
server = "greentrike.cfvgdrxonjze.us-west-2.rds.amazonaws.com",
port = 5432)
#Get list of building that have scores for them
bg.score.3ms.raw <- dbGetQuery(con,'SELECT BGD.bg_geo_id, BGD.variable_id, BGD.Value,BGS.score
FROM "BG_Data" BGD
RIGHT JOIN "BG_Score" BGS ON BGS.bg_geo_id = BGD.bg_geo_id
JOIN "Block_Group" BG on BGD.bg_geo_id = BG.bg_geo_id
WHERE variable_id LIKE \'%_3MS\';')
bg.new.records.3ms <- dbGetQuery(con,'SELECT BGD.bg_geo_id, BGD.variable_id, BGD.Value
FROM "BG_Data" BGD
LEFT JOIN "BG_Score" BGS on BGD.bg_geo_id = BGS.bg_geo_id
JOIN "Block_Group" BG on BGD.bg_geo_id = BG.bg_geo_id
WHERE BGS.bg_geo_id IS NULL
AND variable_id LIKE \'%_3MS\';')
# 3 Mile sum
bg.score.3ms <- reshape2::dcast(bg.score.3ms.raw, bg_geo_id + score ~ variable_id, fun.aggregate = mean)
# bg.score.3ms$score <- (as.factor(bg.score.3ms$score))
bg.score.3ms$score <- (as.integer(bg.score.3ms$score))
#bg.score.3ms$city_short <- as.factor(bg.score.3ms$city_short)
train_index <- sample(row.names(bg.score.3ms),0.6*dim(bg.score.3ms)[1])
valid_index <- setdiff(row.names(bg.score.3ms), train_index)
train.df <- bg.score.3ms[train_index,]
valid.df <- bg.score.3ms[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,3:26], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 3:26] <- predict(norm.values, train.df[,3:26])
valid.norm.df[, 3:26] <- predict(norm.values, valid.df[,3:26])
k.num <-nrow(train.df)
bg.3ms.rmse.df <- data.frame(k = seq(1, k.num, 1), RMSE_value = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k=i)
bg.3ms.rmse.df[i, 2] <- RMSE(as.numeric(as.character(knn.pred)), valid.norm.df[,2])
}
min(bg.3ms.rmse.df)
bg.3ms.rmse.df
### Looks like k = is the highest consistency
knn.model <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$score))
RMSE(as.numeric(knn.model), valid.norm.df[,2])
### Looks like k = is the highest consistency
knn.model <- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$score))
### Looks like k = is the highest consistency
knn.model.train <- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.train), as.factor(train.norm.df$score))
knn.model.valid <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$score))
train_index <- sample(row.names(bg.score.3ms),0.6*dim(bg.score.3ms)[1])
valid_index <- setdiff(row.names(bg.score.3ms), train_index)
train.df <- bg.score.3ms[train_index,]
valid.df <- bg.score.3ms[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,3:26], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 3:26] <- predict(norm.values, train.df[,3:26])
valid.norm.df[, 3:26] <- predict(norm.values, valid.df[,3:26])
k.num <-nrow(train.df)
bg.3ms.rmse.df <- data.frame(k = seq(1, k.num, 1), RMSE_value = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k=i)
bg.3ms.rmse.df[i, 2] <- RMSE(as.numeric(as.character(knn.pred)), valid.norm.df[,2])
}
min(bg.3ms.rmse.df)
bg.3ms.rmse.df
### Looks like k = is the highest consistency
knn.model.train <- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.train), as.factor(train.norm.df$score))
knn.model.valid <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model), as.factor(valid.norm.df$score))
train_index <- sample(row.names(bg.score.3ms),0.6*dim(bg.score.3ms)[1])
valid_index <- setdiff(row.names(bg.score.3ms), train_index)
train.df <- bg.score.3ms[train_index,]
valid.df <- bg.score.3ms[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,3:26], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 3:26] <- predict(norm.values, train.df[,3:26])
valid.norm.df[, 3:26] <- predict(norm.values, valid.df[,3:26])
k.num <-nrow(train.df)
bg.3ms.rmse.df <- data.frame(k = seq(1, k.num, 1), RMSE_value = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k=i)
bg.3ms.rmse.df[i, 2] <- RMSE(as.numeric(as.character(knn.pred)), valid.norm.df[,2])
}
min(bg.3ms.rmse.df)
bg.3ms.rmse.df
### Looks like k = is the highest consistency
knn.model.train <- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.train), as.factor(train.norm.df$score))
knn.model.valid <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.valid), as.factor(valid.norm.df$score))
RMSE(as.numeric(knn.model), valid.norm.df[,2])
train_index <- sample(row.names(bg.score.3ms),0.6*dim(bg.score.3ms)[1])
valid_index <- setdiff(row.names(bg.score.3ms), train_index)
train.df <- bg.score.3ms[train_index,]
valid.df <- bg.score.3ms[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,3:26], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 3:26] <- predict(norm.values, train.df[,3:26])
valid.norm.df[, 3:26] <- predict(norm.values, valid.df[,3:26])
k.num <-nrow(train.df)
bg.3ms.rmse.df <- data.frame(k = seq(1, k.num, 1), RMSE_value = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k=i)
bg.3ms.rmse.df[i, 2] <- RMSE(as.numeric(as.character(knn.pred)), valid.norm.df[,2])
}
min(bg.3ms.rmse.df)
bg.3ms.rmse.df
### Looks like k = is the highest consistency
knn.model.train <- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.train), as.factor(train.norm.df$score))
knn.model.valid <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.valid), as.factor(valid.norm.df$score))
RMSE(as.numeric(knn.model), valid.norm.df[,2])
train_index <- sample(row.names(bg.score.3ms),0.6*dim(bg.score.3ms)[1])
valid_index <- setdiff(row.names(bg.score.3ms), train_index)
train.df <- bg.score.3ms[train_index,]
valid.df <- bg.score.3ms[valid_index,]
# Normalize after split.
# Just run preprocess on training data to get training mean and SD for normalization
norm.values <- preProcess(train.df[,3:26], method = c("center","scale"))
# Create normalized df first so you can omit the correct columns
train.norm.df <- train.df
valid.norm.df <- valid.df
#Insert normalized data.  Notice how you are essentially predicting the Standardization since we have the mean and SD captured already.
train.norm.df[, 3:26] <- predict(norm.values, train.df[,3:26])
valid.norm.df[, 3:26] <- predict(norm.values, valid.df[,3:26])
k.num <-nrow(train.df)
bg.3ms.rmse.df <- data.frame(k = seq(1, k.num, 1), RMSE_value = rep(0, k.num))
# compute knn for different k on validation.
for(i in 1:k.num) {
knn.pred<- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k=i)
bg.3ms.rmse.df[i, 2] <- RMSE(as.numeric(as.character(knn.pred)), valid.norm.df[,2])
}
min(bg.3ms.rmse.df)
bg.3ms.rmse.df
### Looks like k = is the highest consistency
knn.model.train <- class::knn(train = train.norm.df[,3:26], test = train.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.train), as.factor(train.norm.df$score))
knn.model.valid <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =8)
confusionMatrix(as.factor(knn.model.valid), as.factor(valid.norm.df$score))
RMSE(as.numeric(knn.model), valid.norm.df[,2])
bg.score.3ms[,3:26]
bg.score.3ms.norm <- predict(norm.values, bg.score.3ms[,3:26])
# Predict new records for
#Set test name
model_id ="3MS_KNN_v.4"
##Normalize entire scored dataset
bg.score.3ms.norm <- bg.score.3ms
bg.score.3ms.norm[,3:26] <- predict(norm.values, bg.score.3ms[,3:26])
#Unmelt new records
bg.new.records.3ms <- reshape2::dcast(bg.new.records.3ms, bg_geo_id ~ variable_id, fun.aggregate = mean)
head(bg.new.records.3ms)
#Normalize new records
bg.new.records.3ms.norm <- bg.new.records.3ms
bg.new.records.3ms.norm[,-1] <- predict(norm.values, bg.new.records.3ms[,-1])
knn.model <- class::knn(train = train.norm.df[,3:26], test = valid.norm.df[,3:26], cl = train.norm.df[,2],k =4)
knn.pred.new <- class::knn(train = bg.score.3ms.norm[,3:26], test = bg.new.records.3ms.norm[,2:25], cl = bg.score.3ms.norm[,2] ,k=8)
summary(knn.pred.new)
knn.model.total <- class::knn(train = bg.score.3ms.norm[,3:26], test = bg.new.records.3ms.norm[,2:25], cl = bg.score.3ms.norm[,2], k = 8)
knn.model.total
