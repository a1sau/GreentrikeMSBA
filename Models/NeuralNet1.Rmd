---
title: "NeuralNet1"
author: "Enrique Otanez"
date: "3/3/2021"
output: word_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("D:/Templates/UW Stuff/Classes/MSBA/Classes/PM Stuff for me/NeuralNet"))
```

```{r}
setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/PM Stuff for me/NeuralNet")

#### Table 11.2
library(neuralnet)
library(tidyverse)
building <- read.csv("NeuralNetSet.csv")



building$Tacoma <- building$City == "Tacoma"
building$Puyallup <- building$City == "Puyallup"
building$Land <- building$Property_Type == "Land"
building$Multifamily <- building$Property_Type == "Multifamily"
building$Industrial <- building$Property_Type == "Industrial"
building$Office <- building$Property_Type == "Office"
building$Retail <- building$Property_Type == "Retail"
building$Investment <- building$Sale_Type == "Investment"
building$OwnerUser <- building$Sale_Type == "OwnerUser"
building$InvestmentNNN <- building$Sale_Type == "InvestmentNNN"
building$Investment_or_Owner_User <- building$Sale_Type == "Investment or Owner User"

#df$Acceptance <- as.factor(df$Acceptance)
#df$Like <- as.factor(df$Like)
#$Dislike <- as.factor(df$Dislike)

building

building$Price <- (building$Price - min(building$Price))/(max(building$Price) - min(building$Price))
building$SquareFeet <- (building$SquareFee - min(building$SquareFee))/(max(building$SquareFee) - min(building$SquareFee))
building$Price.SQFT <- (building$Price.SQFT - min(building$Price.SQFT))/(max(building$Price.SQFT) - min(building$Price.SQFT))
building$Score <- (building$Score - min(building$Score))/(max(building$Score) - min(building$Score))
building$Tacoma <- building$Tacoma*1
building$Puyallup <- building$Puyallup*1
building$Land <- building$Land*1
building$Multifamily <- building$Multifamily*1
building$Industrial <- building$Industrial*1
building$Office <- building$Office*1
building$Retail <- building$Retail*1
building$Investment <- building$Investment*1
building$OwnerUser <- building$OwnerUser*1
building$InvestmentNNN <- building$InvestmentNNN*1
building$Investment_or_Owner_User <- building$Investment_or_Owner_User*1

#check for duplicates
duplicated(building$Price)

#remove rows with duplicates
building <- building[!duplicated(building[ , "Price"]),]

building

#make neuralnet
set.seed(1)
bld_neun <- neuralnet(Score ~ Price + SquareFeet + Price.SQFT + Tacoma + Puyallup + Land + Multifamily + Industrial + Office + Retail + Investment + OwnerUser + InvestmentNNN + Investment_or_Owner_User, data = building, linear.output = T, hidden = 4)

bld_neun

# display weights
bld_neun$weights

# display predictions
predict_bld <- prediction(bld_neun)
predict_bld

#extracting data from array
predict_bld_score <- predict_bld$rep1[,15]
predict_bld_score

#to add it to the dataset
building$Score.Predict <- predict_bld_score

building

# plot network
plot(bld_neun, rep="best")

actual = c(building$Score)

library(Metrics)
neunet_bld.RMSE <- rmse(building$Score, predict_bld_score)
neunet_bld.RMSE
```


```{r}
#take that model and use it on a training set
#creating the training and validation sets


bld_training = sort(sample(nrow(building), nrow(building)*0.6))
bld_train <- building[bld_training, ]
bld_valid <- building[-bld_training, ]
bld_train

#applying the model to the training set
set.seed(3)
neun_bld_train1 <- neuralnet(Score ~ Price.SQFT + Land + SquareFeet + Tacoma + Industrial + Multifamily + Office + Investment + OwnerUser, data = bld_train, linear.output = T, hidden = c(5,1), act.fct = "logistic", rep = 10)


# display train predictions
predict_bld_train1 <- prediction(neun_bld_train1)
predict_bld_train1

#extracting data from array
predict_score_bld_train1 <- predict_bld_train1$rep1[,10]
predict_score_bld_train1

#to add it to the dataset
bld_train$Score.Predict <- predict_score_bld_train1

bld_train

# plot network
plot(neun_bld_train1, rep = "best")


library(Metrics)
neunet_bld_train1.RMSE <- rmse(bld_train$Score, predict_score_bld_train1)
neunet_bld_train1.RMSE

#One thing to note, although RMSE is high, we can still count on that the SSE is still very very low, meaning that the generalized capability of the model is doing extremely well.










#now use it on the validation set
set.seed(3)
neun_bld_valid1 <- neuralnet(Score ~ Price.SQFT + Land + SquareFeet + Tacoma + Industrial + Multifamily + Office + Investment + OwnerUser, data = bld_valid, linear.output = T, hidden = c(5,1), act.fct = "logistic", rep = 10)

# display valid predictions
predict_bld_valid1 <- prediction(neun_bld_valid1)
predict_bld_valid1

#extracting data from array
predict_score_bld_valid1 <- predict_bld_valid1$rep1[,10]
predict_score_bld_valid1

#to add it to the dataset
bld_valid$Score.Predict <- predict_score_bld_valid1

bld_valid

# plot network
plot(neun_bld_valid1, rep="best")

library(Metrics)
neunet_bld_valid1.RMSE <- rmse(bld_valid$Score, predict_score_bld_valid1)
neunet_bld_valid1.RMSE

#For this first prediction, I am guessing that the RMSE is so drastically larger because of the difference in data. The law of large numbers and such.












#DISCLAIMER. I originally forgot to make a traininga and validation set for this, so basically, its not tuned at all right now. 
#rescale for tanh activation function


bld_train_tanh <- building
summary(bld_train_tanh$Price)

bld_train_tanh$Price <- (2 * ((bld_train_tanh$Price - min(bld_train_tanh$Price))/max(bld_train_tanh$Price) - min(bld_train_tanh$Price))) - 1
bld_train_tanh$SquareFeet <- (2 * ((bld_train_tanh$SquareFeet - min(bld_train_tanh$SquareFeet))/max(bld_train_tanh$SquareFeet) - min(bld_train_tanh$SquareFeet))) - 1
bld_train_tanh$Price.SQFT <- (2 * ((bld_train_tanh$Price.SQFT - min(bld_train_tanh$Price.SQFT))/max(bld_train_tanh$Price.SQFT) - min(bld_train_tanh$Price.SQFT))) - 1
bld_train_tanh$Score <- (2 * ((bld_train_tanh$Score - min(bld_train_tanh$Score))/max(bld_train_tanh$Score) - min(bld_train_tanh$Score))) - 1
bld_train_tanh$Tacoma <- (2 * ((bld_train_tanh$Tacoma - min(bld_train_tanh$Tacoma))/max(bld_train_tanh$Tacoma) - min(bld_train_tanh$Tacoma))) - 1
bld_train_tanh$Puyallup <- (2 * ((bld_train_tanh$Puyallup - min(bld_train_tanh$Puyallup))/max(bld_train_tanh$Puyallup) - min(bld_train_tanh$Puyallup))) - 1
bld_train_tanh$Land <- (2 * ((bld_train_tanh$Land - min(bld_train_tanh$Land))/max(bld_train_tanh$Land) - min(bld_train_tanh$Land))) - 1
bld_train_tanh$Multifamily <- (2 * ((bld_train_tanh$Multifamily - min(bld_train_tanh$Multifamily))/max(bld_train_tanh$Multifamily) - min(bld_train_tanh$Multifamily))) - 1
bld_train_tanh$Industrial <- (2 * ((bld_train_tanh$Industrial - min(bld_train_tanh$Industrial))/max(bld_train_tanh$Industrial) - min(bld_train_tanh$Industrial))) - 1
bld_train_tanh$Office <- (2 * ((bld_train_tanh$Office - min(bld_train_tanh$Office))/max(bld_train_tanh$Office) - min(bld_train_tanh$Office))) - 1
bld_train_tanh$Retail <- (2 * ((bld_train_tanh$Retail - min(bld_train_tanh$Retail))/max(bld_train_tanh$Retail) - min(bld_train_tanh$Retail))) - 1
bld_train_tanh$Investment <- (2 * ((bld_train_tanh$Investment - min(bld_train_tanh$Investment))/max(bld_train_tanh$Investment) - min(bld_train_tanh$Investment))) - 1
bld_train_tanh$OwnerUser <- (2 * ((bld_train_tanh$OwnerUser - min(bld_train_tanh$OwnerUser))/max(bld_train_tanh$OwnerUser) - min(bld_train_tanh$OwnerUser))) - 1
bld_train_tanh$InvestmentNNN <- (2 * ((bld_train_tanh$InvestmentNNN - min(bld_train_tanh$InvestmentNNN))/max(bld_train_tanh$InvestmentNNN) - min(bld_train_tanh$InvestmentNNN))) - 1
bld_train_tanh$Investment_or_Owner_User <- (2 * ((bld_train_tanh$Investment_or_Owner_User - min(bld_train_tanh$Investment_or_Owner_User))/max(bld_train_tanh$Investment_or_Owner_User) - min(bld_train_tanh$Investment_or_Owner_User))) - 1

#remove the value with NaN values. I don't know why they are there, but I do not believe it has much predictive power, so dropping it is the easier choice. 

bld_train_tanh_true <- subset(bld_train_tanh, select = -c(InvestmentNNN))
bld_train_tanh_true


#make training and validation sets
bld_training_tanh = sort(sample(nrow(bld_train_tanh_true), nrow(bld_train_tanh_true)*0.6))
bld_train_tanh <- bld_train_tanh_true[bld_training_tanh, ]
bld_valid_tanh <- bld_train_tanh_true[-bld_training_tanh, ]
bld_train_tanh


#applying the tanh model to the training set
set.seed(4)
neun_bld_train2 <- neuralnet(Score ~ Price + SquareFeet + Price.SQFT + Tacoma + Puyallup + Office + Land + Industrial + OwnerUser, data = bld_train_tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 4)

#display weights
neun_bld_train2$weights

# display train predictions
predict_bld_train2 <- prediction(neun_bld_train2)
predict_bld_train2

#extracting data from array
predict_score_bld_train2 <- predict_bld_train2$rep1[,10]
predict_score_bld_train2

#to add it to the dataset
bld_train_tanh$Score.Predict <- predict_score_bld_train2

bld_train_tanh

# plot network
plot(neun_bld_train2, rep = "best")


library(Metrics)
neunet_bld_train2.RMSE <- rmse(bld_train_tanh$Score, predict_score_bld_train2)
neunet_bld_train2.RMSE

#moving on, this tanh function isn't proving to be super useful. I think maybe it can get better, I got as low as .2 for the error rate but the RMSE still stands at a ~.6 which is not good. I won't be testing the model on the validation set at this point in time. 
```



```{r}
#We do the exact same process as building for census
setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/PM Stuff for me/NeuralNet")

census <- read.csv("NeunetCensusSet.csv")

census


#check for duplicates
duplicated(census$Population)

#remove rows with duplicates
census <- census[!duplicated(census[ , "Population"]),]

census


str(census)

#Change all variables into a single scale
#This is because percentages cannot compare to whole numbers and vice versa, its best to just normalize everything
census$Population <- (census$Population - min(census$Population))/(max(census$Population) - min(census$Population))

census$Population..3.Miles <- (census$Population..3.Miles - min(census$Population..3.Miles))/(max(census$Population..3.Miles) - min(census$Population..3.Miles))

census$Households..3.Miles <- (census$Households..3.Miles - min(census$Households..3.Miles))/(max(census$Households..3.Miles) - min(census$Households..3.Miles))

census$Kids.under.5 <- (census$Kids.under.5 - min(census$Kids.under.5))/(max(census$Kids.under.5) - min(census$Kids.under.5))

census$Kids.under.5..3.Miles <- (census$Kids.under.5..3.Miles - min(census$Kids.under.5..3.Miles))/(max(census$Kids.under.5..3.Miles) - min(census$Kids.under.5..3.Miles))

census$Kids.5.to.9 <- (census$Kids.5.to.9 - min(census$Kids.5.to.9))/(max(census$Kids.5.to.9) - min(census$Kids.5.to.9))

census$Kids.under.5..3.Miles <- (census$Kids.under.5..3.Miles - min(census$Kids.under.5..3.Miles))/(max(census$Kids.under.5..3.Miles) - min(census$Kids.under.5..3.Miles))

census$Average.Age <- (census$Average.Age - min(census$Average.Age))/(max(census$Average.Age) - min(census$Average.Age))

census$Household.income.under.40K..3.Mile <- (census$Household.income.under.40K..3.Mile - min(census$Household.income.under.40K..3.Mile))/(max(census$Household.income.under.40K..3.Mile) - min(census$Household.income.under.40K..3.Mile))

census$Household.income.40K.to.50K..3.Mile <- (census$Household.income.40K.to.50K..3.Mile - min(census$Household.income.40K.to.50K..3.Mile))/(max(census$Household.income.40K.to.50K..3.Mile) - min(census$Household.income.40K.to.50K..3.Mile))

census$Household.income.50K.to.60K..3.Mile <- (census$Household.income.50K.to.60K..3.Mile - min(census$Household.income.50K.to.60K..3.Mile))/(max(census$Household.income.50K.to.60K..3.Mile) - min(census$Household.income.50K.to.60K..3.Mile))

census$Household.income.60K.to.75K..3.Mile <- (census$Household.income.60K.to.75K..3.Mile - min(census$Household.income.60K.to.75K..3.Mile))/(max(census$Household.income.60K.to.75K..3.Mile) - min(census$Household.income.60K.to.75K..3.Mile))

census$Household.income.75K.to.100K..3.Mile <- (census$Household.income.75K.to.100K..3.Mile - min(census$Household.income.75K.to.100K..3.Mile))/(max(census$Household.income.75K.to.100K..3.Mile) - min(census$Household.income.75K.to.100K..3.Mile))

census$Household.income.100K.to.125K..3.Mile <- (census$Household.income.100K.to.125K..3.Mile - min(census$Household.income.100K.to.125K..3.Mile))/(max(census$Household.income.100K.to.125K..3.Mile) - min(census$Household.income.100K.to.125K..3.Mile))

census$Household.income.125K.to.150K..3.Mile <- (census$Household.income.125K.to.150K..3.Mile - min(census$Household.income.125K.to.150K..3.Mile))/(max(census$Household.income.125K.to.150K..3.Mile) - min(census$Household.income.125K.to.150K..3.Mile))

census$Household.income.150K.to.200K..3.Mile <- (census$Household.income.150K.to.200K..3.Mile - min(census$Household.income.150K.to.200K..3.Mile))/(max(census$Household.income.150K.to.200K..3.Mile) - min(census$Household.income.150K.to.200K..3.Mile))

census$Household.income.200K...3.Mile <- (census$Household.income.200K...3.Mile - min(census$Household.income.200K...3.Mile))/(max(census$Household.income.200K...3.Mile) - min(census$Household.income.200K...3.Mile))

census$Area.Score <- (census$Area.Score - min(census$Area.Score))/(max(census$Area.Score) - min(census$Area.Score))

census


#make neuralnet
set.seed(2)
census_neun <- neuralnet(Area.Score ~ Population + Kids.5.to.9 + Household.income.40K.to.50K..3.Mile, data = census, linear.output = T, hidden = 4)

census_neun

# display weights
census_neun$weights

# display predictions
predict_census <- prediction(census_neun)
predict_census

#extracting data from array
predict_score_census <- predict_census$rep1[,4]
predict_score_census

#to add it to the dataset
census$Score.Predict <- predict_score_census

census

# plot network
plot(census_neun, rep="best")

actual = c(census$Area.Score)

library(Metrics)
census_neun.RMSE <- rmse(census$Area.Score, predict_score_census)
census_neun.RMSE
```

```{r}
#Now we make training and validation sets for census

#DISCLAIMER. Like the previous ones, I found ways to calculate the error because once you add more and more values, the plot sooner or later excludes it from the visualization. So I am limiting my inputs to show me the error for now. 

cens_training = sort(sample(nrow(census), nrow(census)*0.6))
cens_train <- census[cens_training, ]
cens_valid <- census[-cens_training, ]
cens_train
cens_valid

#applying the model to the training set
set.seed(5)
neun_cens_train1 <- neuralnet(Area.Score ~ Population + Population..3.Miles + Households..3.Miles + Kids.under.5 + Kids.under.5..3.Miles + Kids.5.to.9 + Average.Age + Household.income.40K.to.50K..3.Mile + Household.income.50K.to.60K..3.Mile + Household.income.60K.to.75K..3.Mile, data = cens_train, linear.output = T, hidden = c(4,1), rep = 10)


# display train weights
neun_cens_train1$weights

# display train predictions
predict_cens_train1 <- prediction(neun_cens_train1)
predict_cens_train1

#extracting data from array
predict_score_cens_train1 <- predict_cens_train1$rep1[,11]
predict_score_cens_train1

#to add it to the dataset
cens_train$Score.Predict <- predict_score_cens_train1

cens_train

# plot network
plot(neun_cens_train1, rep="best")


library(Metrics)
neunet_cens_train1.RMSE <- rmse(cens_train$Area.Score, predict_score_cens_train1)
neunet_cens_train1.RMSE







#lets apply this to a validation set
set.seed(5)
neun_cens_valid1 <- neuralnet(Area.Score ~ Population + Population..3.Miles + Households..3.Miles + Kids.under.5 + Kids.under.5..3.Miles + Kids.5.to.9 + Average.Age + Household.income.40K.to.50K..3.Mile + Household.income.50K.to.60K..3.Mile + Household.income.60K.to.75K..3.Mile, data = cens_valid, linear.output = T, hidden = c(4,1), rep = 10)


# display train weights
neun_cens_valid1$weights

# display train predictions
predict_cens_valid1 <- prediction(neun_cens_valid1)
predict_cens_valid1

#extracting data from array
predict_score_cens_valid1 <- predict_cens_valid1$rep1[,11]
predict_score_cens_valid1

#to add it to the dataset
cens_valid$Score.Predict <- predict_score_cens_valid1

cens_valid

# plot network
plot(neun_cens_valid1, rep="best")


library(Metrics)
neunet_cens_valid1.RMSE <- rmse(cens_valid$Area.Score, predict_score_cens_valid1)
neunet_cens_valid1.RMSE

#so as we can see, the SSE error is much better, almost cut down by half, most likely due to the smaller amount of variables. Maybe because of this again, like I argued the previous model for building, this may be the reason why the RMSE is bigger. This time however, it is not as big of a difference as the building model.





#lets try and work the census model with the tanh function.
cens_tanh <- census
cens_tanh

#set scale from 0 to 1 to -1 to 1
cens_tanh$Population <- (2 * ((cens_tanh$Population - min(cens_tanh$Population))/max(cens_tanh$Population) - min(cens_tanh$Population))) - 1
cens_tanh$Population..3.Miles <- (2 * ((cens_tanh$Population..3.Miles - min(cens_tanh$Population..3.Miles))/max(cens_tanh$Population..3.Miles) - min(cens_tanh$Population..3.Miles))) - 1
cens_tanh$Households..3.Miles <- (2 * ((cens_tanh$Households..3.Miles - min(cens_tanh$Households..3.Miles))/max(cens_tanh$Households..3.Miles) - min(cens_tanh$Households..3.Miles))) - 1
cens_tanh$Kids.under.5 <- (2 * ((cens_tanh$Kids.under.5 - min(cens_tanh$Kids.under.5))/max(cens_tanh$Kids.under.5) - min(cens_tanh$Kids.under.5))) - 1
cens_tanh$Kids.under.5..3.Miles <- (2 * ((cens_tanh$Kids.under.5..3.Miles - min(cens_tanh$Kids.under.5..3.Miles))/max(cens_tanh$Kids.under.5..3.Miles) - min(cens_tanh$Kids.under.5..3.Miles))) - 1
cens_tanh$Kids.5.to.9 <- (2 * ((cens_tanh$Kids.5.to.9 - min(cens_tanh$Kids.5.to.9))/max(cens_tanh$Kids.5.to.9) - min(cens_tanh$Kids.5.to.9))) - 1
cens_tanh$Kids.5.to.9..3.Miles <- (2 * ((cens_tanh$Kids.5.to.9..3.Miles - min(cens_tanh$Kids.5.to.9..3.Miles))/max(cens_tanh$Kids.5.to.9..3.Miles) - min(cens_tanh$Kids.5.to.9..3.Miles))) - 1
cens_tanh$Average.Age <- (2 * ((cens_tanh$Average.Age - min(cens_tanh$Average.Age))/max(cens_tanh$Average.Age) - min(cens_tanh$Average.Age))) - 1
cens_tanh$Household.income.under.40K..3.Mile <- (2 * ((cens_tanh$Household.income.under.40K..3.Mile - min(cens_tanh$Household.income.under.40K..3.Mile))/max(cens_tanh$Household.income.under.40K..3.Mile) - min(cens_tanh$Household.income.under.40K..3.Mile))) - 1
cens_tanh$Household.income.40K.to.50K..3.Mile <- (2 * ((cens_tanh$Household.income.40K.to.50K..3.Mile - min(cens_tanh$Household.income.40K.to.50K..3.Mile))/max(cens_tanh$Household.income.40K.to.50K..3.Mile) - min(cens_tanh$Household.income.40K.to.50K..3.Mile))) - 1
cens_tanh$Household.income.50K.to.60K..3.Mile <- (2 * ((cens_tanh$Household.income.50K.to.60K..3.Mile - min(cens_tanh$Household.income.50K.to.60K..3.Mile))/max(cens_tanh$Household.income.50K.to.60K..3.Mile) - min(cens_tanh$Household.income.50K.to.60K..3.Mile))) - 1
cens_tanh$Household.income.60K.to.75K..3.Mile <- (2 * ((cens_tanh$Household.income.60K.to.75K..3.Mile - min(cens_tanh$Household.income.60K.to.75K..3.Mile))/max(cens_tanh$Household.income.60K.to.75K..3.Mile) - min(cens_tanh$Household.income.60K.to.75K..3.Mile))) - 1
cens_tanh$Household.income.75K.to.100K..3.Mile <- (2 * ((cens_tanh$Household.income.75K.to.100K..3.Mile - min(cens_tanh$Household.income.75K.to.100K..3.Mile))/max(cens_tanh$Household.income.75K.to.100K..3.Mile) - min(cens_tanh$Household.income.75K.to.100K..3.Mile))) - 1
cens_tanh$Household.income.100K.to.125K..3.Mile <- (2 * ((cens_tanh$Household.income.100K.to.125K..3.Mile - min(cens_tanh$Household.income.100K.to.125K..3.Mile))/max(cens_tanh$Household.income.100K.to.125K..3.Mile) - min(cens_tanh$Household.income.100K.to.125K..3.Mile))) - 1
cens_tanh$Household.income.125K.to.150K..3.Mile <- (2 * ((cens_tanh$Household.income.125K.to.150K..3.Mile - min(cens_tanh$Household.income.125K.to.150K..3.Mile))/max(cens_tanh$Household.income.125K.to.150K..3.Mile) - min(cens_tanh$Household.income.125K.to.150K..3.Mile))) - 1
cens_tanh$Household.income.150K.to.200K..3.Mile <- (2 * ((cens_tanh$Household.income.150K.to.200K..3.Mile - min(cens_tanh$Household.income.150K.to.200K..3.Mile))/max(cens_tanh$Household.income.150K.to.200K..3.Mile) - min(cens_tanh$Household.income.125K.to.150K..3.Mile))) - 1
cens_tanh$Household.income.200K...3.Mile <- (2 * ((cens_tanh$Household.income.200K...3.Mile - min(cens_tanh$Household.income.200K...3.Mile))/max(cens_tanh$Household.income.200K...3.Mile) - min(cens_tanh$Household.income.200K...3.Mile))) - 1

cens_tanh

#make training and validation sets
cens_training_tanh = sort(sample(nrow(cens_tanh), nrow(cens_tanh)*0.6))
cens_train_tanh <- cens_tanh[cens_training_tanh, ]
cens_valid_tanh <- cens_tanh[-cens_training_tanh, ]
cens_train_tanh


#lets make a model with tanh!
set.seed(6)
neun_cens_train2 <- neuralnet(Area.Score ~ Population + Population..3.Miles + Households..3.Miles + Kids.under.5 + Kids.under.5..3.Miles + Kids.5.to.9 + Average.Age + Household.income.40K.to.50K..3.Mile + Household.income.50K.to.60K..3.Mile + Household.income.60K.to.75K..3.Mile, data = cens_train_tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 10)


#display weights
neun_cens_train2$weights

# display train predictions
predict_cens_train2 <- prediction(neun_cens_train2)
predict_cens_train2

#extracting data from array
predict_score_cens_train2 <- predict_cens_train2$rep1[,11]
predict_score_cens_train2

#to add it to the dataset
cens_train_tanh$Score.Predict <- predict_score_cens_train2

cens_train_tanh

# plot network
plot(neun_cens_train2, rep = "best")


library(Metrics)
neunet_cens_train2.RMSE <- rmse(cens_train_tanh$Area.Score, predict_score_cens_train2)
neunet_cens_train2.RMSE

#To be honest, its 3AM and i just copied the model from the earlier one, and this things is pretty good. SSE of .08 i think and RMSE of .2 something. Im moving on to the validation set. 

set.seed(6)
neun_cens_valid2 <- neuralnet(Area.Score ~ Population + Population..3.Miles + Households..3.Miles + Kids.under.5 + Kids.under.5..3.Miles + Kids.5.to.9 + Average.Age + Household.income.40K.to.50K..3.Mile + Household.income.50K.to.60K..3.Mile + Household.income.60K.to.75K..3.Mile, data = cens_valid_tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 10)


#display weights
neun_cens_valid2$weights

# display train predictions
predict_cens_valid2 <- prediction(neun_cens_valid2)
predict_cens_valid2

#extracting data from array
predict_score_cens_valid2 <- predict_cens_valid2$rep1[,11]
predict_score_cens_valid2

#to add it to the dataset
cens_valid_tanh$Score.Predict <- predict_score_cens_valid2

cens_valid_tanh

# plot network
plot(neun_cens_valid2, rep = "best")


library(Metrics)
neunet_cens_valid2.RMSE <- rmse(cens_tanh$Area.Score, predict_score_cens_valid2)
neunet_cens_valid2.RMSE
```

