---
title: "NeuralNet1"
author: "Enrique Otanez"
date: "3/3/2021"
output: word_document
editor_options: 
  chunk_output_type: inline
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models/Tuning/NuralNet"))
```

```{r}
#make a folder named something that makes sense
#file name example type_model_date.csv or building_model8_20210409.csv
#output a csv to folder where python can pull it and put it into the database
#you want only two columns the record "cs_id" and the predicted score
#include a signifier building vs census (make it in the name)
library(DBI)
library(sqldf)
library(dplyr)
#library(tidyverse)
library(neuralnet)
library(Metrics)
library(caret)
library(config)
library(datapackage.r)
library(jsonlite)
library(fastDummies)
library(datarium)
library(ggplot2)
```

```{r}
setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models")
df <- read.csv("Config_File.csv")

con <- DBI::dbConnect(odbc::odbc(),
  driver = "PostgreSQL Unicode(x64)",
  database = "TEST",
  UID      = df$UID,
  PWD      = df$PWD,
  server = df$server,
  port = 5432)
```

```{r}
import.building <- dbGetQuery(con, 'SELECT
    bld."CS_ID"
    ,avg(bs."Score") "Average Building Score"
    ,bld."Address_Line"
    ,bld."City"
    ,bld."Postal_Code"
    ,bld."Property_Type"
    ,bld."Year_Built"
    ,bld."Price"
    ,bld."SquareFeet"
    ,round(cast(coalesce(bld."Price" / bld."SquareFeet",NULL) as numeric),0) "$ per sq ft"
     ,bld."Sale_Type"
     ,bg.bg_geo_id "Block Group ID"
     ,avg(bgs.score) "Average Block Group Score"
     ,max(case when dv.sid=\'pop\' then bgd.value Else 0 END) "Population"
     ,max(case when dv.sid=\'pop_MF_3MS\' then bgd.value Else 0 END) "Population: 3 Miles"
     ,max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) "Households: 3 Miles"
    ,max(case when dv.sid=\'M_0_5\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5\' then bgd.value Else 0 END) "Kids under 5"
     ,round(cast((max(case when dv.sid=\'M_0_5\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5"\' then bgd.value Else 0 END)) /
        max(case when dv.sid=\'pop\' then bgd.value Else null END) as numeric),3) "Percent Kids under 5"
    ,max(case when dv.sid=\'M_0_5_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5_3MS\' then bgd.value Else 0 END) "Kids under 5: 3 Miles"
     ,round(cast((max(case when dv.sid=\'M_0_5_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5_3MS\' then bgd.value Else 0 END)) /
        max(case when dv.sid=\'pop_MF_3MS\' then bgd.value Else null END) as numeric),3) "Percent Kids under 5: 3 Miles"
    ,max(case when dv.sid=\'M_5_9\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9\' then bgd.value Else 0 END) "Kids 5 to 9"
     ,round(cast((max(case when dv.sid=\'M_5_9\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9\' then bgd.value Else 0 END)) /
        max(case when dv.sid=\'pop\' then bgd.value Else null END) as numeric),3) "Percent Kids 5 to 9"
    ,max(case when dv.sid=\'M_5_9_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9_3MS\' then bgd.value Else 0 END) "Kids 5 to 9: 3 Miles"
    ,round(cast((max(case when dv.sid=\'M_5_9_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9_3MS\' then bgd.value Else 0 END)) /
         max(case when dv.sid=\'pop_MF_3MS\' then bgd.value Else null END) as numeric),3)  "Percent Kids 5 to 9: 3 Miles"
    ,max(case when dv.sid=\'avg_age\' then bgd.value Else 0 END) "Average Age"
    ,round(cast(sum(case when dv.sid in(\'hi_0_10_3MS\',\'hi_10_15_3MS\',\'hi_15_20_3MS\',\'hi_20_25_3MS\',\'hi_25_30_3MS\',\'hi_30_35_3MS\',\'hi_35_40_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income under 40K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_40_45_3MS\',\'hi_45_50_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 40K to 50K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_50_60_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 50K to 60K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_60_75_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 60K to 75K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_75_100_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 75K to 100K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_100_125_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 100K to 125K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_125_150_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 125K to 150K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_150_200_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 150K to 200K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_200_999_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 200K+: 3 Mile"
from "Building" as bld
left join "Block_Group" as bg on bg.bg_geo_id = bld.bg_geo_id
left join "BG_Data" as bgd on bg.bg_geo_id = bgd.bg_geo_id
inner join "Demo_Var" as dv on dv.full_variable_id=bgd.variable_id
left join "BG_Score" as bgs on bg.bg_geo_id = bgs.bg_geo_id
left join "Building_Score" as bs on bld."CS_ID" = bs.cs_id
group by bld."CS_ID",bld."Address_Line",bld."City",bld."Postal_Code",bld."Property_Type",bld."Price",bld."Year_Built",bld."SquareFeet",bld."Sale_Type",bg.bg_geo_id
having
    max(case when dv.sid=\'pop\' then bgd.value Else 0 END) > 0
    and max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) > 0')

import.building
```


```{r}
#We need to query the variables used for the score as well as the score from the DB

building.scores <- sqldf('SELECT CS_ID, "Average Building Score", Address_Line, City, Postal_Code, Property_Type, Year_Built, Price, SquareFeet, "$ per sq ft", Sale_Type FROM "import.building" WHERE "Average Building Score" IS NOT NULL')

building.scores
```

```{r}
#Remove any and all NA's
building.scores <- na.omit(building.scores)
#Check for remaining NA's
sum(is.na(building.scores))
#View dataset
building.scores

sum(is.na(building.scores))
```

```{r}
#Caution needs to be considered when modeling. Dummy coding both city and postal code is fine, but using them together in the same model might create
#multicollinearity issues as they represent more or less the same thing. It would be best to create two modesl using either city or postal code
#and see which one is the better predictor
building.scores <- fastDummies::dummy_cols(building.scores, select_columns = "City")
building.scores <- fastDummies::dummy_cols(building.scores, select_columns = "Postal_Code")
building.scores <- fastDummies::dummy_cols(building.scores, select_columns = "Property_Type")
building.scores <- fastDummies::dummy_cols(building.scores, select_columns = "Sale_Type")
building.scores
```
```{r}
#now lets drop some rows we do not need anymore
building.scores <- subset(building.scores, select = -c(City, Postal_Code, Property_Type, Year_Built, Sale_Type, Property_Type_IndustrialOfficeIndustrialLiveWorkUnit, `Property_Type_OfficeOfficeGeneral Retail Convenience StoreGeneral Retail RestaurantGeneral Retail Storefront Retail/OfficeOffice`, `Property_Type_OfficeOfficeGeneral Retail Daycare CenterGeneral Retail StorefrontGeneral Retail Storefront Retail/OfficeOffice`, `Sale_Type_Investment NNN`, `Sale_Type_InvestmentorOwnerUser`, Sale_Type_OwnerUser))
building.scores
```


```{r}
#Now to normalize all numeric variables but not the score yet, we want to make a csv for exporting. 
building.scores$Price <- (building.scores$Price - min(building.scores$Price))/(max(building.scores$Price) - min(building.scores$Price))

building.scores$SquareFeet <- (building.scores$SquareFeet - min(building.scores$SquareFeet))/(max(building.scores$SquareFeet) - min(building.scores$SquareFeet))

building.scores$`$ per sq ft` <- (building.scores$`$ per sq ft` - min(building.scores$`$ per sq ft`))/(max(building.scores$`$ per sq ft`) - min(building.scores$`$ per sq ft`))

building.scores
```


```{r}
#now normalize the score
building.scores$`Average Building Score` <- (building.scores$`Average Building Score` - min(building.scores$`Average Building Score`))/(max(building.scores$`Average Building Score`) - min(building.scores$`Average Building Score`))

building.scores
```


```{r}
#check for duplicates
duplicated(building.scores$Price)

#remove rows with duplicates
building.scores <- building.scores[!duplicated(building.scores[ , "Price"]),]

building.scores
```
```{r}
#rename some variables to work in the model, names can't have spaces
names(building.scores)[names(building.scores) == "$ per sq ft"] <- "price_per_sq_ft"
names(building.scores)[names(building.scores) == "Sale_Type_Investment or Owner User"] <- "Sale_Type_Investment_or_Owner_User"
names(building.scores)[names(building.scores) == "Sale_Type_Owner User"] <- "Sale_Type_Owner_User"
names(building.scores)[names(building.scores) == "City_Gig Harbor"] <- "City_Gig_Harbor"
building.scores
```


```{r}
building.tanh <- building.scores
building.tanh
```

```{r}
#make the csv for exporting
bld.export <- building.scores
bld.export
```

```{r}
#to properly use rep, summ the reps and then divide by the reps (basically average just smarter)
building.scores
#creating the training and validation sets
bld.training = sort(sample(nrow(building.scores), nrow(building.scores)*0.6))
bld.train <- building.scores[bld.training, ]
bld.valid <- building.scores[-bld.training, ]
bld.train
bld.valid
```


```{r}
#applying the model to the training set
set.seed(3)
neun.bld.train1 <- neuralnet(`Average Building Score` ~ Price + SquareFeet + City_Tacoma + City_Puyallup + Property_Type_Industrial + Property_Type_Office + Sale_Type_Investment + Sale_Type_Investment_or_Owner_User + Sale_Type_Owner_User, data = bld.train, linear.output = T, hidden = c(5,1), act.fct = "logistic")
```


```{r}
# display train predictions
predict.bld.train1 <- prediction(neun.bld.train1)
predict.bld.train1

#extracting data from array
predict.score.bld.train1 <- predict.bld.train1$rep1[,10]
predict.score.bld.train1

#to add it to the dataset
bld.train$Score.Predict <- predict.score.bld.train1

bld.train

# plot network
plot(neun.bld.train1)
```


```{r}
#calculate RMSE
neunet.bld.train1.RMSE <- rmse(bld.train$`Average Building Score`, bld.train$Score.Predict)
neunet.bld.train1.RMSE

#denormalize score
bld.train$Score.Predict.UNNorm <- bld.train$Score.Predict * 5

#One thing to note, although RMSE is high, we can still count on that the SSE is still very very low, meaning that the generalized capability of the model is doing extremely well.
```
```{r}
#training confusion matrix
#bld.train.actual <- as.factor(round(bld.train$`Average Building Score` * 5))
#bld.train.actual

#bld.train.predict <- as.factor(round(bld.train$Score.Predict * 5))
#bld.train.predict

#bld.train.cm <- confusionMatrix(data = bld.train.actual, reference = bld.train.predict)
```



```{r}
#now use it on the validation set
set.seed(3)
neun.bld.valid1 <- neuralnet(`Average Building Score` ~ Price + SquareFeet + City_Tacoma + City_Puyallup + Property_Type_Industrial + Property_Type_Office + Sale_Type_Investment + Sale_Type_Investment_or_Owner_User + Sale_Type_Owner_User, data = bld.valid, linear.output = T, hidden = c(5,1), act.fct = "logistic")
```


```{r}
# display valid predictions
predict.bld.valid1 <- prediction(neun.bld.valid1)
predict.bld.valid1

#extracting data from array
predict.score.bld.valid1 <- predict.bld.valid1$rep1[,10]
predict.score.bld.valid1

#to add it to the dataset
bld.valid$Score.Predict <- predict.score.bld.valid1

bld.valid

# plot network

plot(neun.bld.valid1)
```


```{r}
#denormalize predicted score

bld.valid$Score.Predict.UNNorm <- bld.valid$Score.Predict * 5

#calculate RMSE for Validation Set
bld.valid1.RMSE <- rmse(bld.valid$`Average Building Score`, bld.valid$Score.Predict)
bld.valid1.RMSE

```

```{r}
#validation confusion matrix
bld.valid.actual <- as.factor(round(bld.valid$`Average Building Score` * 5))
bld.valid.actual

bld.valid.predict <- as.factor(round(bld.valid$Score.Predict * 5))
bld.valid.predict

bld.valid.cm <- confusionMatrix(data = bld.valid.actual, reference = bld.valid.predict)

bld.valid.cm
```


```{r}
#plotting predicted score and actual score against price
ggplot1 <- ggplot(bld.train, aes(x = Price, y = Score.Predict.UNNorm)) + geom_point() + geom_smooth(method = NULL)
plot(ggplot1)
```
```{r}
ggplot2 <- ggplot(bld.valid, aes(x = Price, y = Score.Predict)) + geom_point() + geom_smooth(method = NULL)
plot(ggplot2)
```


```{r}
#lift chart

```


```{r}
#learning curve
#https://stackoverflow.com/questions/38582076/how-to-plot-a-learning-curve-in-r
set.seed(3)
learnCurve <- data.frame(m = integer(21),
                         trainRMSE = integer(21),
                         cvRMSE = integer(21))
testY <- bld.valid$`Average Building Score`
trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
metric <- "RMSE"

for (i in 3:21) {
  learnCurve$m[i] <- i
  fit.nn <- neuralnet(`Average Building Score` ~ 
                        Price + SquareFeet + price_per_sq_ft + City_Tacoma + City_Puyallup + 
                        Property_Type_Industrial + Property_Type_Office + Property_Type_Retail + 
                        Sale_Type_Investment + Sale_Type_Investment_or_Owner_User + Sale_Type_Owner_User, 
                        data = bld.train[1:i,], linear.output = T, hidden = c(5,1), act.fct = "logistic")
  learnCurve$trainRMSE[i] <- neunet.bld.train1.RMSE
  
  prediction <- predict(fit.nn, newdata = bld.valid[,-1])
  rmse <- postResample(prediction, testY)
  learnCurve$cvRMSE[i] <- rmse[1]
}

pdf("LinearRegressionLearningCurve.pdf", width = 7, height = 7, pointsize = 12)
```


```{r}
plot(log(learnCurve$trainRMSE), type = "o", col = "red", xlab = "Training set size",
     ylab = "Error (RMSE)", main = "Neural Network Learning Curve")
lines(log(learnCurve$cvRMSE), type = "o", col = "blue")
legend('topright', c("Train error", "Test error"), lty = c(1,1), lwd = c(2.5, 2.5),
       col = c("red", "blue"))


```


```{r}
#learning curve attempt 2
set.seed(3)
ctrl <- trainControl(classProbs = TRUE,
                     summaryFunction = twoClassSummary)

lda_data <- learning_curve_dat(dat = bld.train, outcome = "Average Building Score", test_prop = 1/4, method = "", metric = "ROC", trControl = ctrl)
```


```{r}
#To make the loop work, the variables not being used or do not have much predictive power can be dropped from the building.tanh dataframe.
building.tanh <- building.tanh[-c(3:7)]
building.tanh <- building.tanh[-c(6)]
building.tanh
#lets try and work the building model with the tanh function.

#Set scale from -1 to 1
tanh.norm <- function(x) {
  (2 * (x - min(x)) / (max(x) - min(x))) - 1
}

for (i in 2:length(building.tanh)) {
  building.tanh[i] <- tanh.norm(building.tanh[i])
}

view(building.tanh)

#remove the value with NaN values. I don't know why they are there, but I do not believe it has much predictive power, so dropping it is the easier choice. 
building.tanh <- subset(building.tanh, select = -c(InvestmentNNN, Land, Multifamily))
building.tanh
```


```{r}
#Check for NA's
sum(is.na(building.tanh))

#Make training and validation set
bld.training.tanh = sort(sample(nrow(building.tanh), nrow(building.tanh)*0.6))
bld.train.tanh <- building.tanh[bld.training.tanh, ]
bld.valid.tanh <- building.tanh[-bld.training.tanh, ]
bld.train.tanh
```


```{r}
#applying the tanh model to the training set
set.seed(4)
neun.bld.train2 <- neuralnet(`Average Building Score` ~ Price + SquareFeet + Retail + Investment + Tacoma + Puyallup + Office + Industrial + OwnerUser, data = bld.train.tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 4)

# display train predictions
predict.bld.train2 <- prediction(neun.bld.train2)
predict.bld.train2

#extracting data from array
predict.score.bld.train2 <- predict.bld.train2$rep1[,10]
predict.score.bld.train2

#to add it to the dataset
bld.train.tanh$Score.Predict <- predict.score.bld.train2

bld.train.tanh

# plot network
plot(neun.bld.train2, rep = "best")
```


```{r}
#denormalize predicted score

bld.train.tanh$Score.Predict.UNNorm <- (bld.train.tanh$Score.Predict * 2) + 3
bld.train.tanh

#calculate RMSE for Validation Set
bld.train.tanh.RMSE <- rmse(bld.train.tanh$`Average Building Score`, bld.train.tanh$Score.Predict)
bld.train.tanh.RMSE

#moving on, this tanh function isn't proving to be super useful. I think maybe it can get better, I got as low as .2 for the error rate but the RMSE still stands at a ~.6 which is not good.
```
```{r}
#applying the tanh model to the training set
set.seed(4)
neun.bld.valid2 <- neuralnet(`Average Building Score` ~ Price + SquareFeet + Retail + Investment + Tacoma + Puyallup + Office + Industrial + OwnerUser, data = bld.valid.tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 4)

# display train predictions
predict.bld.valid2 <- prediction(neun.bld.valid2)
predict.bld.valid2

#extracting data from array
predict.score.bld.valid2 <- predict.bld.valid2$rep1[,10]
predict.score.bld.valid2

#to add it to the dataset
bld.valid.tanh$Score.Predict <- predict.score.bld.valid2

bld.valid.tanh

# plot network
plot(neun.bld.valid2, rep = "best")
```
```{r}
#denormalize predicted score

bld.valid.tanh$Score.Predict.UNNorm <- (bld.valid.tanh$Score.Predict * 2) + 3
bld.valid.tanh

#calculate RMSE for Validation Set
bld.valid.tanh.RMSE <- rmse(bld.valid.tanh$`Average Building Score`, bld.valid.tanh$Score.Predict)
bld.valid.tanh.RMSE
```


```{r}
#The chosen model to be repeated and made into an output.
set.seed(3)
bld.neun <- neuralnet(`Average Building Score` ~ Price + Land + SquareFeet + Tacoma + Industrial + Multifamily + Office + Investment + OwnerUser, data = building.scores, linear.output = T, hidden = c(5,1), act.fct = "logistic", rep = 10)

#save the model
setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models/Tuning/NuralNet")
save(file="FinalBuildingNeunetModel",bld.neun)
#This is to use for later use when we switch over to Final_NeuNet_Enrique.
```


```{r}
# display predictions
predict.bld <- prediction(bld.neun)
predict.bld

#extracting data from array
predict.bld.score <- predict.bld$rep1[,10]
predict.bld.score

#to add it to the dataset
building.scores$Score.Predict <- predict.bld.score

building.scores

# plot network
plot(bld.neun, rep="best")
```


```{r}
#denormalize predicted score
building.scores$Score.Predict.UNNorm <- building.scores$Score.Predict * 5
building.scores

bld.RMSE <- rmse(building.scores$`Average Building Score`, building.scores$Score.Predict)
bld.RMSE
```


```{r}
#check for duplicates
duplicated(bld.export$Price)

#remove rows with duplicates
bld.export <- bld.export[!duplicated(bld.export[ , "Price"]),]

#add predicted scores to export csv
bld.export$Score.Predict.Norm <- building.scores$Score.Predict.UNNorm

bld.export

setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models/Tuning/NuralNet")
write.csv(bld.export, "D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models/Tuning/NuralNet\\NNTestBuildingScore.csv", row.names = FALSE)
```