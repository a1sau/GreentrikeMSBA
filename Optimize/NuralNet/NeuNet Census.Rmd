---
title: "NeuNet Census"
author: "Enrique Otanez"
date: "4/8/2021"
output: word_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("D:/Templates/UW Stuff/Classes/MSBA/Classes/PM Stuff for me/NeuralNet"))
```

```{r}
library(DBI)
library(sqldf)
library(tidyverse)
library(neuralnet)
library(Metrics)
library(caret)
```


```{r}
#Connecting R to PostgreSQL
setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models")
df <- read.csv("Config_File.csv")

con <- DBI::dbConnect(odbc::odbc(),
  driver = "PostgreSQL Unicode(x64)",
  database = "TEST",
  UID      = df$UID,
  PWD      = df$PWD,
  server = df$server,
  port = 5432)
```

```{r}
import.census <- dbGetQuery(con, 'select
     bg.bg_geo_id "Block Group ID"
     ,(select avg(bgs.score) from "BG_Score" as bgs where bgs.bg_geo_id=bg.bg_geo_id) "Average Block Group Score"
     ,max(case when dv.sid=\'pop\' then bgd.value Else 0 END) "Population"
     ,max(case when dv.sid=\'pop_MF_3MS\' then bgd.value Else 0 END) "Population: 3 Mile"
     ,max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) "Households: 3 Mile"
    ,max(case when dv.sid=\'M_0_5\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5\' then bgd.value Else 0 END) "Kids under 5"
     ,round(cast((max(case when dv.sid=\'M_0_5\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5\' then bgd.value Else 0 END)) /
        max(case when dv.sid=\'pop\' then bgd.value Else null END) as numeric),3) "Percent Kids under 5"
    ,max(case when dv.sid=\'M_0_5_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5_3MS\' then bgd.value Else 0 END) "Kids under 5: 3 Mile"
     ,round(cast((max(case when dv.sid=\'M_0_5_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_0_5_3MS\' then bgd.value Else 0 END)) /
        max(case when dv.sid=\'pop_MF_3MS\' then bgd.value Else null END) as numeric),3) "Percent Kids under 5: 3 Mile"
    ,max(case when dv.sid=\'M_5_9\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9\' then bgd.value Else 0 END) "Kids 5 to 9"
     ,round(cast((max(case when dv.sid=\'M_5_9\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9\' then bgd.value Else 0 END)) /
        max(case when dv.sid=\'pop\' then bgd.value Else null END) as numeric),3) "Percent Kids 5 to 9"
    ,max(case when dv.sid=\'M_5_9_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9_3MS\' then bgd.value Else 0 END) "Kids 5 to 9: 3 Mile"
    ,round(cast((max(case when dv.sid=\'M_5_9_3MS\' then bgd.value Else 0 END)+max(case when dv.sid=\'F_5_9_3MS\' then bgd.value Else 0 END)) /
         max(case when dv.sid=\'pop_MF_3MS\' then bgd.value Else null END) as numeric),3)  "Percent Kids 5 to 9: 3 Mile"
    ,max(case when dv.sid=\'avg_age\' then bgd.value Else 0 END) "Average Age"
    ,round(cast(sum(case when dv.sid in(\'hi_0_10_3MS\',\'hi_10_15_3MS\',\'hi_15_20_3MS\',\'hi_20_25_3MS\',\'hi_25_30_3MS\',\'hi_30_35_3MS\',\'hi_35_40_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income under 40K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_40_45_3MS\',\'hi_45_50_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 40K to 50K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_50_60_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 50K to 60K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_60_75_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 60K to 75K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_75_100_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 75K to 100K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_100_125_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 100K to 125K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_125_150_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 125K to 150K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_150_200_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 150K to 200K: 3 Mile"
    ,round(cast(sum(case when dv.sid in(\'hi_200_999_3MS\') then bgd.value  else 0 END) /
      max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END) as numeric),3) "Household income 200K+: 3 Mile"
from "Block_Group" as bg
left join "BG_Data" as bgd on bg.bg_geo_id = bgd.bg_geo_id
inner join "Demo_Var" as dv on dv.full_variable_id=bgd.variable_id
group by bg.bg_geo_id
having
max(case when dv.sid=\'pop\' then bgd.value Else 0 END) > 0
and max(case when dv.sid=\'hi_tot_3MS\' then bgd.value Else 0 END)>0')
import.census
```
```{r}
#We need to query the variables used for the score as well as the score from the DB

census.scores <- sqldf('SELECT "Block Group ID" AS "Block.Group.ID", "Average Block Group Score" AS "Average.Block.Group.Score", Population, "Population: 3 Mile" AS "Population.3.Mile", "Households: 3 Mile" AS "Households.3.Mile", "Percent Kids under 5" AS "Percent.Kids.under.5", "Percent Kids under 5: 3 Mile" AS "Percent.Kids.under.5.3.Mile", "Percent Kids 5 to 9" AS "Percent.Kids.5.to.9", "Percent Kids 5 to 9: 3 Mile" AS "Percent.Kids.5.to.9.3 Mile", "Average Age" AS "Average.Age", "Household income under 40K: 3 Mile" AS "Household.income.under.40K.3Mile", "Household income 40K to 50K: 3 Mile" AS "Household.income.40K.to.50K.3.Mile", "Household income 50K to 60K: 3 Mile" AS "Household.income.50K.to.60K.3.Mile", "Household income 60K to 75K: 3 Mile" AS "Household.income.60K.to.75K.3.Mile", "Household income 75K to 100K: 3 Mile" AS "Household.income.75K.to.100K.3.Mile", "Household income 100K to 125K: 3 Mile" AS "Household.income.100:.125K.3.Mile", "Household income 125K to 150K: 3 Mile" AS "Household.income.125K.to.150K.3.Mile", "Household income 150K to 200K: 3 Mile" AS "Household.income.150K.to.200K.3.Mile", "Household income 200K+: 3 Mile"  AS "Household.income.200K+.3Mile"FROM "import.census" WHERE "Average Block Group Score" IS NOT NULL')

census.scores
#this is to make an export of the final model output
census.export <- census.scores
```

```{r}
#Change all variables into a single scale
#This is because percentages cannot compare to whole numbers and vice versa, its best to just normalize everything
min.max.norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

for (i in 2:length(census.scores)) {
  census.scores[i] <- min.max.norm(census.scores[i])
}

view(census.scores)
```


```{r}
#Now we make training and validation sets for census

#DISCLAIMER. Like the previous ones, I found ways to calculate the error because once you add more and more values, the plot sooner or later excludes it from the visualization. So I am limiting my inputs to show me the error for now. 

cens.training = sort(sample(nrow(census.scores), nrow(census.scores)*0.6))
cens.train <- census.scores[cens.training, ]
cens.valid <- census.scores[-cens.training, ]
cens.train
cens.valid
```


```{r}
#applying the model to the training set
set.seed(5)
neun.cens.train1 <- neuralnet(Average.Block.Group.Score ~ Population + Population.3.Mile + Households.3.Mile + Percent.Kids.under.5 + Percent.Kids.under.5.3.Mile + Percent.Kids.5.to.9 + Average.Age + Household.income.40K.to.50K.3.Mile + Household.income.50K.to.60K.3.Mile + Household.income.60K.to.75K.3.Mile, data = cens.train, linear.output = T, hidden = c(4,1), rep = 10)

# display train predictions
predict.cens.train1 <- prediction(neun.cens.train1)
predict.cens.train1

#extracting data from array
predict.score.cens.train1 <- predict.cens.train1$rep1[,11]
predict.score.cens.train1

#to add it to the dataset
cens.train$Score.Predict <- predict.score.cens.train1

cens.train

# plot network
plot(neun.cens.train1, rep="best")
```


```{r}
#get RMSE error
neunet.cens.train1.RMSE <- Metrics::rmse(cens.train$Average.Block.Group.Score, cens.train$Score.Predict)
neunet.cens.train1.RMSE
```


```{r}
#lets apply this to a validation set
set.seed(5)
neun.cens.valid1 <- neuralnet(Average.Block.Group.Score ~ Population + Population.3.Mile + Households.3.Mile + Percent.Kids.under.5 + Percent.Kids.under.5.3.Mile + Percent.Kids.5.to.9 + Average.Age + Household.income.40K.to.50K.3.Mile + Household.income.50K.to.60K.3.Mile + Household.income.60K.to.75K.3.Mile, data = cens.valid, linear.output = T, hidden = c(4,1), rep = 10)

# display train predictions
predict.cens.valid1 <- prediction(neun.cens.valid1)
predict.cens.valid1

#extracting data from array
predict.score.cens.valid1 <- predict.cens.valid1$rep1[,11]
predict.score.cens.valid1

#to add it to the dataset
cens.valid$Score.Predict <- predict.score.cens.valid1

cens.valid

# plot network
plot(neun.cens.valid1, rep="best")
```


```{r}
neunet.cens.valid1.RMSE <- rmse(cens.valid$Average.Block.Group.Score, cens.valid$Score.Predict)
neunet.cens.valid1.RMSE

#so as we can see, the SSE error is much better, almost cut down by half, most likely due to the smaller amount of variables. Maybe because of this again, like I argued the previous model for building, this may be the reason why the RMSE is bigger. This time however, it is not as big of a difference as the building model.
```


```{r}
#lets try and work the census model with the tanh function.
census.tanh <- census.export
census.tanh

#Set scale from -1 to 1
tanh.norm <- function(x) {
  (2 * (x - min(x)) / (max(x) - min(x))) - 1
}

for (i in 2:length(census.tanh)) {
  census.tanh[i] <- tanh.norm(census.tanh[i])
}

view(census.tanh)
```


```{r}
#make training and validation sets
cens.training.tanh = sort(sample(nrow(census.tanh), nrow(census.tanh)*0.6))
cens.train.tanh <- census.tanh[cens.training.tanh, ]
cens.valid.tanh <- census.tanh[-cens.training.tanh, ]
cens.train.tanh
```


```{r}
#lets make a model with tanh!
set.seed(6)
neun.cens.train2 <- neuralnet(Average.Block.Group.Score ~ Population + Population.3.Mile + Households.3.Mile + Percent.Kids.under.5 + Percent.Kids.under.5.3.Mile + Percent.Kids.5.to.9 + Average.Age + Household.income.40K.to.50K.3.Mile + Household.income.50K.to.60K.3.Mile + Household.income.60K.to.75K.3.Mile, data = cens.train.tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 10)

# display train predictions
predict.cens.train2 <- prediction(neun.cens.train2)
predict.cens.train2

#extracting data from array
predict.score.cens.train2 <- predict.cens.train2$rep1[,11]
predict.score.cens.train2

#to add it to the dataset
cens.train.tanh$Score.Predict <- predict.score.cens.train2

cens.train.tanh

# plot network
plot(neun.cens.train2, rep = "best")
```


```{r}
neunet.cens.train2.RMSE <- rmse(cens.train.tanh$Average.Block.Group.Score, cens.train.tanh$Score.Predict)
neunet.cens.train2.RMSE

confusionMatrix(as.factor(round(cens.train.tanh$Score.Predict,0)),as.factor(cens.train.tanh$Average.Block.Group.Score))
```


```{r}
set.seed(6)
neun.cens.valid2 <- neuralnet(Average.Block.Group.Score ~ Population + Population.3.Mile + Households.3.Mile + Percent.Kids.under.5 + Percent.Kids.under.5.3.Mile + Percent.Kids.5.to.9 + Average.Age + Household.income.40K.to.50K.3.Mile + Household.income.50K.to.60K.3.Mile + Household.income.60K.to.75K.3.Mile, data = cens.valid.tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 10)

# display train predictions
predict.cens.valid2 <- prediction(neun.cens.valid2)
predict.cens.valid2

#extracting data from array
predict.score.cens.valid2 <- predict.cens.valid2$rep1[,11]
predict.score.cens.valid2

#to add it to the dataset
cens.valid.tanh$Score.Predict <- predict.score.cens.valid2

cens.valid.tanh

# plot network
plot(neun.cens.valid2, rep = "best")
```


```{r}
neunet.cens.valid2.RMSE <- rmse(cens.valid.tanh$Average.Block.Group.Score, cens.valid.tanh$Score.Predict)
neunet.cens.valid2.RMSE



confusionMatrix(as.factor(round(cens.valid.tanh$Score.Predict,0)),as.factor(cens.valid.tanh$Average.Block.Group.Score))
```

```{r}
#This is the chosen model and will be ran again on the full dataset of known scores and then an output will be made. 
set.seed(6)
census.neun <- neuralnet(Average.Block.Group.Score ~ Population + Population.3.Mile + Households.3.Mile + Percent.Kids.under.5 + Percent.Kids.under.5.3.Mile + Percent.Kids.5.to.9 + Average.Age + Household.income.40K.to.50K.3.Mile + Household.income.50K.to.60K.3.Mile + Household.income.60K.to.75K.3.Mile, data = census.tanh, linear.output = T, hidden = c(4,1), act.fct = "tanh", rep = 10)

setwd("D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models/Tuning/NuralNet")
save(file="censusneunetmodel",census.neun)

# display predictions
predict.census <- prediction(census.neun)
predict.census

#extracting data from array
predict.score.census <- predict.census$rep1[,11]
predict.score.census

#to add it to the dataset
census.tanh$Score.Predict <- predict.score.census

census.tanh

# plot network
plot(census.neun, rep="best")
```


```{r}
census.neun.RMSE <- rmse(census.tanh$Average.Block.Group.Score, census.tanh$Score.Predict)
census.neun.RMSE
```


```{r}
#Denormalize predicted score
census.tanh$Score.Predict <- (census.tanh$Score.Predict * 2) +3
census.tanh$Score.Predict

census.export$Score.Predict <- census.tanh$Score.Predict
census.export

write.csv(census.export, "D:/Templates/UW Stuff/Classes/MSBA/Classes/Q4 Models/Tuning/NuralNet\\NNTestCensusScore.csv", row.names = FALSE)
```